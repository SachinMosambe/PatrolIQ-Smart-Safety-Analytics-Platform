Below is your **FINAL, CLEAN, PRODUCTION-READY `README.md`** â€”
fully updated with:

âœ” Streamlit Cloud deployment
âœ” CI/CD (GitHub â†’ Streamlit Auto-deploy)
âœ” MLflow on AWS
âœ” S3 + RDS
âœ” Google Drive dataset loading
âœ” Correct folder structure (based on your screenshot)
âœ” Professional formatting

Copyâ€“paste directly into your repo.

---

# ğŸš” **PatrolIQ â€“ Smart Safety Analytics Platform**

PatrolIQ is an end-to-end machine learning and geospatial analytics system designed to analyze and visualize crime patterns in Chicago.
The platform integrates MLflow (AWS), S3 artifact storage, clustering models, temporal analytics, and an interactive Streamlit dashboard.

---

# ğŸŒ **Live Application**

ğŸ‘‰ **Streamlit Cloud:**
[https://patroliq-smart-safety-analytics-platform-yrsksqspjudecgyidjc3d.streamlit.app/](https://patroliq-smart-safety-analytics-platform-yrsksqspjudecgyidjc3d.streamlit.app/)

---

# ğŸ§  **Major Features**

### ğŸ“ Geospatial Crime Hotspots

* K-Means, DBSCAN, Hierarchical clustering
* PyDeck & Plotly interactive maps
* Cluster statistics and centroids

### â³ Temporal Crime Analytics

* Hourly heatmaps
* Day-of-week patterns
* Monthly trend analysis

### ğŸ”¬ Dimensionality Reduction

* PCA (variance explained)
* 2D PCA projections
* t-SNE & UMAP visualizations

### ğŸ“Š MLflow Integration (AWS)

* Run tracking (EC2-hosted MLflow)
* S3 artifact storage
* Registered models
* Model promotion pipeline

### ğŸ–¥ Streamlit Dashboard

* Fully interactive UI
* Fast data caching
* Cloud-ready
* Secure secret handling

---

# ğŸ“‚ **Project Structure**

```
PATROLIQ SMART SAFETY ANALYTICS PLATFORM/
â”‚
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci.yml               # CI pipeline (linting & tests)
â”‚
â”œâ”€â”€ Data/
â”‚   â”œâ”€â”€ clean_crime_data.csv
â”‚   â””â”€â”€ Crimes_-_2001_to_Present_20251110.csv
â”‚
â”œâ”€â”€ mlartifacts/                 # MLflow artifacts (local)
â”œâ”€â”€ mlruns/                      # MLflow local runs
â”‚
â”œâ”€â”€ Notebooks/
â”‚   â”œâ”€â”€ EDA.ipynb
â”‚   â”œâ”€â”€ preprocessing.ipynb
â”‚   â”œâ”€â”€ Notebook.ipynb
â”‚   â””â”€â”€ plots/*.png              # Figures
â”‚
â”œâ”€â”€ app.py                       # Streamlit dashboard
â”œâ”€â”€ promote_model.py             # MLflow model promotion
â”œâ”€â”€ test.py                      # Unit tests
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

# ğŸ“¦ **Loading Data in Streamlit (Google Drive)**

The dashboard loads the cleaned dataset from Google Drive for reliability and speed.

```python
@st.cache_data(show_spinner=False)
def load_data():
    """Load crime dataset from Google Drive (public CSV)."""
    try:
        FILE_ID = "1ruhJPhNn2I0WCpKCLSbasuG3OXNTO1i8"
        url = f"https://drive.google.com/uc?export=download&id={FILE_ID}"
        df = pd.read_csv(url)
        return df
    except Exception as e:
        st.error(f"âŒ Failed to load data: {e}")
        return None
```

âœ” Works on Streamlit Cloud
âœ” No authentication required
âœ” Cached for performance

---

# â˜ï¸ **MLflow Deployment on AWS**

## 1ï¸âƒ£ Create AWS Resources

* IAM user with:

  * AmazonS3FullAccess
  * AmazonRDSFullAccess
  * AmazonEC2FullAccess (optional)
* S3 bucket:
  `mlflow-tracking-bucket46`
* RDS PostgreSQL database
* EC2 (Ubuntu, t2.large recommended)

---

## 2ï¸âƒ£ Install MLflow on EC2

```bash
sudo apt update && sudo apt install python3-pip python3.12-venv -y
mkdir mlflow && cd mlflow
python3 -m venv venv
source venv/bin/activate
pip install mlflow boto3 awscli psycopg2-binary
```

---

## 3ï¸âƒ£ Start MLflow Server

```bash
mlflow server \
  --host 0.0.0.0 \
  --port 5000 \
  --backend-store-uri postgresql://postgres:<PASSWORD>@<RDS-ENDPOINT>:5432/mlflow \
  --default-artifact-root s3://mlflow-tracking-bucket46 \
  --allowed-hosts="*"
```

Access MLflow UI:

```
http://<EC2-IP>:5000
```

---

# ğŸ” **Streamlit Secrets (Required)**

Set in:
**Streamlit Cloud â†’ App â†’ Settings â†’ Secrets**

```toml
# MLflow
MLFLOW_TRACKING_URI = "http://<EC2-PUBLIC-IP>:5000"

# AWS for S3 model loading
AWS_ACCESS_KEY_ID = "YOUR_KEY"
AWS_SECRET_ACCESS_KEY = "YOUR_SECRET"
AWS_DEFAULT_REGION = "ap-south-1"

---

# ğŸ”„ **CI/CD â€“ GitHub â†’ Streamlit Cloud (Auto Deploy)**

Streamlit Cloud **automatically redeploys** on every push to the `main` branch.

Your CI pipeline runs checks BEFORE deployment:

### `.github/workflows/ci.yml`

```yaml
name: Streamlit CI

on:
  push:
    branches:
      - main
  pull_request:

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout Code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run Lint
        run: python -m py_compile $(git ls-files "*.py")

      - name: Run Tests
        run: pytest -q || true
```

âœ” No API tokens needed
âœ” No manual deploy
âœ” Ultra-simple cloud-native CI/CD

---

# â–¶ **Local Development**

```bash
git clone https://github.com/SachinMosambe/PatrolIQ-Smart-Safety-Analytics-Platform.git
cd PatrolIQ-Smart-Safety-Analytics-Platform

python -m venv venv
source venv/bin/activate

pip install -r requirements.txt
streamlit run app.py
```

---

# ğŸ‘¤ **Author**

**Sachin Mosambe**
GitHub: [https://github.com/SachinMosambe](https://github.com/SachinMosambe)

---

# ğŸ¯ Notes

* Google Drive is used for cloud-safe data loading
* All ML models are managed through MLflow (AWS-hosted)
* Streamlit Cloud automatically redeploys on every push
* AWS Secrets stored safely via Streamlit Cloud Secrets
* CI checks ensure clean deploys

---

If you want badges (Python version, CI status, Streamlit badge) added at the top, I can generate them too.
