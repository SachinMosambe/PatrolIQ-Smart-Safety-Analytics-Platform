{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94024a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.tracking import MlflowClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361677b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_df = pd.read_csv('../Data/clean_crime_data.csv')\n",
    "print(f\"‚úì Loaded {len(crime_df):,} crime records\")\n",
    "print(f\"‚úì Original columns: {crime_df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a26063c",
   "metadata": {},
   "source": [
    "FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1805ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Convert Date to datetime\n",
    "crime_df['Date'] = pd.to_datetime(crime_df['Date'], errors='coerce')\n",
    "\n",
    "# Extract temporal features\n",
    "crime_df['Hour'] = crime_df['Date'].dt.hour\n",
    "crime_df['Day_of_Week'] = crime_df['Date'].dt.dayofweek  # 0=Monday\n",
    "crime_df['Month'] = crime_df['Date'].dt.month\n",
    "crime_df['Year'] = crime_df['Date'].dt.year\n",
    "print(\"‚úì Temporal features: Hour, Day_of_Week, Month, Year\")\n",
    "\n",
    "# Season classification\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]: return 'Winter'\n",
    "    elif month in [3, 4, 5]: return 'Spring'\n",
    "    elif month in [6, 7, 8]: return 'Summer'\n",
    "    else: return 'Fall'\n",
    "\n",
    "crime_df['Season'] = crime_df['Month'].apply(get_season)\n",
    "print(\"‚úì Season feature created\")\n",
    "\n",
    "# Weekend flag\n",
    "crime_df['Is_Weekend'] = crime_df['Day_of_Week'].isin([5, 6]).astype(int)\n",
    "print(\"‚úì Weekend flag created\")\n",
    "\n",
    "# Crime severity score\n",
    "severity_map = {\n",
    "    'HOMICIDE': 10, 'CRIM SEXUAL ASSAULT': 9, 'ROBBERY': 8,\n",
    "    'ASSAULT': 7, 'BATTERY': 7, 'BURGLARY': 6,\n",
    "    'MOTOR VEHICLE THEFT': 6, 'THEFT': 5, 'NARCOTICS': 5,\n",
    "    'CRIMINAL DAMAGE': 4, 'DECEPTIVE PRACTICE': 4, 'WEAPONS VIOLATION': 8\n",
    "}\n",
    "crime_df['Crime_Severity_Score'] = crime_df['Primary Type'].map(severity_map).fillna(3)\n",
    "print(\"‚úì Crime Severity Score created\")\n",
    "\n",
    "# Arrest as binary\n",
    "crime_df['Arrest'] = crime_df['Arrest'].astype(int)\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoders = {}\n",
    "categorical_cols = ['Primary Type', 'Location Description', 'District', 'Season']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in crime_df.columns:\n",
    "        le = LabelEncoder()\n",
    "        crime_df[f'{col}_Encoded'] = le.fit_transform(crime_df[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "\n",
    "print(f\"‚úì Encoded: {categorical_cols}\")\n",
    "\n",
    "# Clean data\n",
    "crime_df = crime_df.dropna(subset=['Latitude', 'Longitude', 'Hour', 'Month'])\n",
    "print(f\"\\n‚úì Final dataset: {crime_df.shape[0]:,} rows √ó {crime_df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4975d0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri('http://ec2-65-2-75-98.ap-south-1.compute.amazonaws.com:5000/')\n",
    "mlflow.set_experiment(\"PatrolIQ_Geographic_Clustering_Experiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1703dab2",
   "metadata": {},
   "source": [
    "MLFLOW SETUP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927d0b7d",
   "metadata": {},
   "source": [
    "GEOGRAPHIC CLUSTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c949f851",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_geo = crime_df[['Latitude', 'Longitude']].copy()\n",
    "scaler_geo = StandardScaler()\n",
    "X_geo_scaled = scaler_geo.fit_transform(X_geo)\n",
    "\n",
    "# ---- ELBOW METHOD ----\n",
    "print(\"\\nüîç Finding optimal K...\")\n",
    "sample_size = min(10000, len(X_geo_scaled))\n",
    "sample_idx = np.random.choice(len(X_geo_scaled), sample_size, replace=False)\n",
    "X_sample = X_geo_scaled[sample_idx]\n",
    "\n",
    "k_range = range(5, 11)\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans_temp = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels_temp = kmeans_temp.fit_predict(X_geo_scaled)\n",
    "    inertias.append(kmeans_temp.inertia_)\n",
    "    sil_score = silhouette_score(X_sample, labels_temp[sample_idx])\n",
    "    silhouette_scores.append(sil_score)\n",
    "    print(f\"  k={k} ‚Üí Silhouette={sil_score:.3f}\")\n",
    "\n",
    "optimal_k = k_range[np.argmax(silhouette_scores)]\n",
    "print(f\"\\n‚úì Optimal K: {optimal_k}\")\n",
    "\n",
    "# Plot Elbow Method\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "axes[0].plot(k_range, inertias, 'bo-')\n",
    "axes[0].set_title('Elbow Method')\n",
    "axes[0].set_xlabel('Number of Clusters')\n",
    "axes[0].set_ylabel('Inertia')\n",
    "axes[0].grid(True)\n",
    "\n",
    "axes[1].plot(k_range, silhouette_scores, 'ro-')\n",
    "axes[1].axhline(y=0.5, color='green', linestyle='--', label='Target 0.5')\n",
    "axes[1].set_title('Silhouette Score vs K')\n",
    "axes[1].set_xlabel('Number of Clusters')\n",
    "axes[1].set_ylabel('Silhouette Score')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('elbow_method.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba65b394",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with mlflow.start_run(run_name='KMeans'):\n",
    "    kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "    kmeans_labels = kmeans.fit_predict(X_geo_scaled)\n",
    "    \n",
    "    kmeans_sil = silhouette_score(X_sample, kmeans_labels[sample_idx])\n",
    "    kmeans_db = davies_bouldin_score(X_geo_scaled, kmeans_labels)\n",
    "    \n",
    "    mlflow.log_param('algorithm', 'KMeans')\n",
    "    mlflow.log_param('n_clusters', optimal_k)\n",
    "    mlflow.log_metric('silhouette_score', kmeans_sil)\n",
    "    mlflow.log_metric('davies_bouldin_score', kmeans_db)\n",
    "    mlflow.log_artifact('elbow_method.png')\n",
    "    mlflow.sklearn.log_model(kmeans, 'model')\n",
    "    \n",
    "    print(f\"  Silhouette: {kmeans_sil:.3f} | Davies-Bouldin: {kmeans_db:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8476d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with mlflow.start_run(run_name='DBSCAN'):\n",
    "    EPS_METERS = 300\n",
    "    MIN_SAMPLES = 10\n",
    "    EARTH_RADIUS = 6371000\n",
    "    \n",
    "    coords_rad = np.deg2rad(X_geo.values)\n",
    "    eps_rad = EPS_METERS / EARTH_RADIUS\n",
    "    \n",
    "    dbscan = DBSCAN(eps=eps_rad, min_samples=MIN_SAMPLES, metric='haversine', n_jobs=-1)\n",
    "    dbscan_labels = dbscan.fit_predict(coords_rad)\n",
    "    \n",
    "    n_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n",
    "    n_noise = np.sum(dbscan_labels == -1)\n",
    "    \n",
    "    if n_clusters >= 2:\n",
    "        valid_idx = np.where(dbscan_labels != -1)[0]\n",
    "        sample_idx_db = np.random.choice(valid_idx, min(10000, len(valid_idx)), replace=False)\n",
    "        dbscan_sil = silhouette_score(coords_rad[sample_idx_db], dbscan_labels[sample_idx_db])\n",
    "        dbscan_db = davies_bouldin_score(coords_rad[valid_idx], dbscan_labels[valid_idx])\n",
    "    else:\n",
    "        dbscan_sil = -1.0\n",
    "        dbscan_db = 999.0\n",
    "    \n",
    "    mlflow.log_param('algorithm', 'DBSCAN')\n",
    "    mlflow.log_param('eps_meters', EPS_METERS)\n",
    "    mlflow.log_param('min_samples', MIN_SAMPLES)\n",
    "    mlflow.log_metric('n_clusters', n_clusters)\n",
    "    mlflow.log_metric('n_noise_points', n_noise)\n",
    "    mlflow.log_metric('silhouette_score', dbscan_sil)\n",
    "    mlflow.log_metric('davies_bouldin_score', dbscan_db)\n",
    "    mlflow.sklearn.log_model(dbscan, 'model')\n",
    "    \n",
    "    print(f\"  Clusters: {n_clusters} | Noise: {n_noise} | Silhouette: {dbscan_sil:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c135d15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with mlflow.start_run(run_name='Hierarchical'):\n",
    "    sample_size_hier = min(5000, len(X_geo_scaled))\n",
    "    idx_hier = np.random.choice(len(X_geo_scaled), sample_size_hier, replace=False)\n",
    "    X_sample_hier = X_geo_scaled[idx_hier]\n",
    "    \n",
    "    hierarchical = AgglomerativeClustering(n_clusters=optimal_k, linkage='ward')\n",
    "    hier_labels_sample = hierarchical.fit_predict(X_sample_hier)\n",
    "    \n",
    "    hier_sil = silhouette_score(X_sample_hier, hier_labels_sample)\n",
    "    hier_db = davies_bouldin_score(X_sample_hier, hier_labels_sample)\n",
    "    \n",
    "    mlflow.log_param('algorithm', 'Hierarchical')\n",
    "    mlflow.log_param('n_clusters', optimal_k)\n",
    "    mlflow.log_param('sample_size', sample_size_hier)\n",
    "    mlflow.log_metric('silhouette_score', hier_sil)\n",
    "    mlflow.log_metric('davies_bouldin_score', hier_db)\n",
    "    \n",
    "    print(f\"  Silhouette: {hier_sil:.3f} | Davies-Bouldin: {hier_db:.3f}\")\n",
    "    \n",
    "    # Dendrogram\n",
    "    dendro_size = min(1000, sample_size_hier)\n",
    "    d_idx = np.random.choice(sample_size_hier, dendro_size, replace=False)\n",
    "    X_dendro = X_sample_hier[d_idx]\n",
    "    \n",
    "    Z = linkage(X_dendro, method='ward')\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    dendrogram(Z, truncate_mode='lastp', p=30)\n",
    "    plt.title(\"Hierarchical Clustering Dendrogram\")\n",
    "    plt.xlabel(\"Cluster Size\")\n",
    "    plt.ylabel(\"Distance\")\n",
    "    plt.savefig(\"dendrogram.png\", dpi=300)\n",
    "    plt.show()\n",
    "    mlflow.log_artifact(\"dendrogram.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6219cbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Algorithm': ['KMeans', 'DBSCAN', 'Hierarchical'],\n",
    "    'Silhouette': [kmeans_sil, dbscan_sil, hier_sil],\n",
    "    'Davies-Bouldin': [kmeans_db, dbscan_db, hier_db],\n",
    "    'Clusters': [optimal_k, n_clusters, optimal_k]\n",
    "})\n",
    "print(comparison_df)\n",
    "\n",
    "valid_sil = comparison_df['Silhouette'].copy()\n",
    "valid_sil[valid_sil < 0] = -999\n",
    "best_algo = comparison_df.loc[valid_sil.idxmax(), 'Algorithm']\n",
    "print(f\"\\nüèÜ Best Algorithm: {best_algo}\")\n",
    "\n",
    "# Add best labels to dataframe\n",
    "if best_algo == 'KMeans':\n",
    "    best_labels = kmeans_labels\n",
    "elif best_algo == 'DBSCAN':\n",
    "    best_labels = dbscan_labels\n",
    "else:\n",
    "    # Predict hierarchical on full dataset\n",
    "    hier_full = AgglomerativeClustering(n_clusters=optimal_k, linkage='ward')\n",
    "    best_labels = hier_full.fit_predict(X_geo_scaled)\n",
    "\n",
    "crime_df['GeoCluster'] = best_labels\n",
    "\n",
    "# Cluster statistics\n",
    "cluster_stats = crime_df.groupby('GeoCluster').agg({\n",
    "    'ID': 'count',\n",
    "    'Primary Type': lambda x: x.value_counts().idxmax(),\n",
    "    'Arrest': 'mean',\n",
    "    'Latitude': 'mean',\n",
    "    'Longitude': 'mean'\n",
    "}).rename(columns={\n",
    "    'ID': 'Total_Crimes',\n",
    "    'Primary Type': 'Dominant_Crime',\n",
    "    'Arrest': 'Arrest_Rate'\n",
    "})\n",
    "\n",
    "print(\"\\nüìç Cluster Statistics:\")\n",
    "print(cluster_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5f7923",
   "metadata": {},
   "source": [
    "TEMPORAL CLUSTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e13ee96",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"PatrolIQ_Temporal_Clustering_Experiment\")\n",
    "\n",
    "X_temporal = crime_df[['Hour', 'Day_of_Week', 'Month']].copy()\n",
    "scaler_temp = StandardScaler()\n",
    "X_temporal_scaled = scaler_temp.fit_transform(X_temporal)\n",
    "\n",
    "with mlflow.start_run(run_name='KMeans_Temporal'):\n",
    "    kmeans_temp = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
    "    temporal_labels = kmeans_temp.fit_predict(X_temporal_scaled)\n",
    "    \n",
    "    sample_idx_temp = np.random.choice(len(X_temporal_scaled), min(5000, len(X_temporal_scaled)), replace=False)\n",
    "    temp_sil = silhouette_score(X_temporal_scaled[sample_idx_temp], temporal_labels[sample_idx_temp])\n",
    "    \n",
    "    mlflow.log_param('algorithm', 'KMeans_Temporal')\n",
    "    mlflow.log_param('n_clusters', 4)\n",
    "    mlflow.log_metric('silhouette_score', temp_sil)\n",
    "    mlflow.sklearn.log_model(kmeans_temp, 'model')\n",
    "    \n",
    "    print(f\"‚úì Silhouette Score: {temp_sil:.3f}\")\n",
    "\n",
    "crime_df['TemporalCluster'] = temporal_labels\n",
    "\n",
    "temporal_profiles = crime_df.groupby('TemporalCluster')[['Hour', 'Day_of_Week', 'Month']].mean()\n",
    "print(\"\\n‚è±Ô∏è  Temporal Cluster Profiles:\")\n",
    "print(temporal_profiles)\n",
    "\n",
    "# Hourly heatmap\n",
    "print(\"\\nüìà Creating hourly heatmap...\")\n",
    "hourly_daily = crime_df.groupby(['Day_of_Week', 'Hour']).size().unstack(fill_value=0)\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.heatmap(hourly_daily, cmap='YlOrRd', cbar_kws={'label': 'Crime Count'})\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.ylabel('Day of Week (0=Monday)')\n",
    "plt.title('Crime Heatmap: Day vs Hour')\n",
    "plt.savefig('hourly_heatmap.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3f107b",
   "metadata": {},
   "source": [
    "DIMENSIONALITY REDUCTION - PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a7de87",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"PatrolIQ_Dimensionality_Reduction_Experiment\")\n",
    "\n",
    "# Prepare feature matrix\n",
    "numeric_features = ['Hour', 'Day_of_Week', 'Month', 'Is_Weekend', \n",
    "                    'Crime_Severity_Score', 'Arrest',\n",
    "                    'Primary Type_Encoded', 'Location Description_Encoded',\n",
    "                    'District_Encoded', 'Season_Encoded']\n",
    "numeric_features = [col for col in numeric_features if col in crime_df.columns]\n",
    "\n",
    "X_all_features = crime_df[numeric_features].copy()\n",
    "scaler_all = StandardScaler()\n",
    "X_all_scaled = scaler_all.fit_transform(X_all_features)\n",
    "\n",
    "with mlflow.start_run(run_name='PCA'):\n",
    "    # Find optimal components for 70% variance\n",
    "    pca_full = PCA()\n",
    "    pca_full.fit(X_all_scaled)\n",
    "    \n",
    "    cumsum_var = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "    n_components_70 = np.argmax(cumsum_var >= 0.70) + 1\n",
    "    \n",
    "    # Final PCA\n",
    "    pca = PCA(n_components=max(3, n_components_70))\n",
    "    X_pca_transformed = pca.fit_transform(X_all_scaled)\n",
    "    \n",
    "    explained_var = pca.explained_variance_ratio_.sum()\n",
    "    \n",
    "    mlflow.log_param('technique', 'PCA')\n",
    "    mlflow.log_param('n_components', pca.n_components_)\n",
    "    mlflow.log_metric('explained_variance', explained_var)\n",
    "    mlflow.log_metric('n_components_70pct', n_components_70)\n",
    "    mlflow.sklearn.log_model(pca, 'model')\n",
    "    \n",
    "    print(f\"‚úì Components: {pca.n_components_} | Variance: {explained_var:.2%}\")\n",
    "\n",
    "# Scree plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, len(pca_full.explained_variance_ratio_) + 1),\n",
    "         pca_full.explained_variance_ratio_, 'bo-')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('PCA Scree Plot')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(cumsum_var) + 1), cumsum_var, 'ro-')\n",
    "plt.axhline(y=0.70, color='green', linestyle='--', label='70% Target')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Variance')\n",
    "plt.title('Cumulative Explained Variance')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('pca_scree_plot.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Top features\n",
    "feature_importance = np.abs(pca.components_).sum(axis=0)\n",
    "top_features = pd.DataFrame({\n",
    "    'Feature': numeric_features,\n",
    "    'Importance': feature_importance\n",
    "}).sort_values('Importance', ascending=False)\n",
    "print(\"\\nüîù Top 5 Important Features:\")\n",
    "print(top_features.head(5))\n",
    "\n",
    "# PCA 2D visualization\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(X_pca_transformed[:, 0], X_pca_transformed[:, 1], \n",
    "            c=temporal_labels, cmap='viridis', s=10, alpha=0.5)\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\n",
    "plt.title('PCA 2D Visualization')\n",
    "plt.colorbar(label='Temporal Cluster')\n",
    "plt.savefig('pca_2d.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Reconstruction error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "X_pca_reconstructed = pca.inverse_transform(X_pca_transformed)\n",
    "reconstruction_error = mean_squared_error(X_all_scaled, X_pca_reconstructed)\n",
    "print(f\"‚úì Reconstruction Error: {reconstruction_error:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfeecfa",
   "metadata": {},
   "source": [
    "t-SNE VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cc8a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with mlflow.start_run(run_name='t-SNE'):\n",
    "    sample_size_tsne = min(5000, len(X_all_scaled))\n",
    "    sample_indices = np.random.choice(len(X_all_scaled), sample_size_tsne, replace=False)\n",
    "    X_sample_tsne = X_all_scaled[sample_indices]\n",
    "    \n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "    X_tsne = tsne.fit_transform(X_sample_tsne)\n",
    "    \n",
    "    mlflow.log_param('technique', 't-SNE')\n",
    "    mlflow.log_param('n_components', 2)\n",
    "    mlflow.log_param('perplexity', 30)\n",
    "    mlflow.log_param('sample_size', sample_size_tsne)\n",
    "    \n",
    "    print(f\"‚úì t-SNE completed on {sample_size_tsne:,} samples\")\n",
    "\n",
    "# t-SNE plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "temp_labels_sample = temporal_labels[sample_indices]\n",
    "axes[0].scatter(X_tsne[:, 0], X_tsne[:, 1], c=temp_labels_sample, \n",
    "                cmap='viridis', s=20, alpha=0.6)\n",
    "axes[0].set_title('t-SNE: Colored by Temporal Cluster')\n",
    "axes[0].set_xlabel('t-SNE 1')\n",
    "axes[0].set_ylabel('t-SNE 2')\n",
    "\n",
    "hour_sample = crime_df.iloc[sample_indices]['Hour'].values\n",
    "axes[1].scatter(X_tsne[:, 0], X_tsne[:, 1], c=hour_sample, \n",
    "                cmap='coolwarm', s=20, alpha=0.6)\n",
    "axes[1].set_title('t-SNE: Colored by Hour of Day')\n",
    "axes[1].set_xlabel('t-SNE 1')\n",
    "axes[1].set_ylabel('t-SNE 2')\n",
    "plt.colorbar(axes[1].collections[0], ax=axes[1], label='Hour')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('tsne_visualization.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e029d903",
   "metadata": {},
   "source": [
    "MODEL REGISTRY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c3987a",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MlflowClient()\n",
    "\n",
    "experiment = client.get_experiment_by_name(\"PatrolIQ_Geographic_Clustering_Experiment\")\n",
    "runs = client.search_runs(experiment_ids=[experiment.experiment_id])\n",
    "best_run = max(runs, key=lambda r: r.data.metrics.get('silhouette_score', 0))\n",
    "\n",
    "print(f\"Best Run ID: {best_run.info.run_id}\")\n",
    "print(f\"Algorithm: {best_run.data.params.get('algorithm')}\")\n",
    "print(f\"Silhouette Score: {best_run.data.metrics.get('silhouette_score'):.3f}\")\n",
    "\n",
    "model_uri = f\"runs:/{best_run.info.run_id}/model\"\n",
    "result = mlflow.register_model(model_uri, \"PatrolIQ_Best_Model\")\n",
    "print(f\"\\n‚úÖ Model Registered: {result.name} (Version {result.version})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6314115",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = client.get_experiment_by_name(\"PatrolIQ_Temporal_Clustering_Experiment\")\n",
    "runs = client.search_runs(experiment_ids=[exp.experiment_id])\n",
    "\n",
    "best_temporal = max(runs, key=lambda r: r.data.metrics.get('silhouette_score', 0))\n",
    "temp_uri = f\"runs:/{best_temporal.info.run_id}/model\"\n",
    "\n",
    "temp_reg = mlflow.register_model(temp_uri, \"PatrolIQ_Temporal_Clustering_Model\")\n",
    "print(\"Temporal Registered Version =\", temp_reg.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00457470",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = client.get_experiment_by_name(\"PatrolIQ_Dimensionality_Reduction_Experiment\")\n",
    "runs = client.search_runs(experiment_ids=[exp.experiment_id])\n",
    "\n",
    "best_pca = max(runs, key=lambda r: r.data.metrics.get('explained_variance', 0))\n",
    "pca_uri = f\"runs:/{best_pca.info.run_id}/model\"\n",
    "\n",
    "pca_reg = mlflow.register_model(pca_uri, \"PatrolIQ_Dimensionality_Reduction_Model\")\n",
    "print(\"PCA Registered Version =\", pca_reg.version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fa0eb1",
   "metadata": {},
   "source": [
    "FINAL SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f964446",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ PROJECT COMPLETED - ALL REQUIREMENTS MET\")\n",
    "print(\"=\"*80)\n",
    "print(\"‚úì Feature Engineering (Hour, Day, Month, Season, Weekend, Severity)\")\n",
    "print(\"‚úì 3 Clustering Algorithms (K-Means, DBSCAN, Hierarchical)\")\n",
    "print(\"‚úì Elbow Method for optimal K\")\n",
    "print(\"‚úì Dendrogram for Hierarchical Clustering\")\n",
    "print(\"‚úì Temporal Clustering with 4 patterns\")\n",
    "print(\"‚úì Hourly Crime Heatmap\")\n",
    "print(f\"‚úì PCA with {pca.n_components_} components ({explained_var:.1%} variance)\")\n",
    "print(\"‚úì PCA Scree Plot\")\n",
    "print(\"‚úì t-SNE 2D Visualization\")\n",
    "print(\"‚úì MLflow Experiment Tracking\")\n",
    "print(\"‚úì Model Registry (Best Model Registered)\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
